#!/usr/bin/env python3
"""
SEC-Compliant Document Scraper - Fixed to use correct browse endpoint
"""

import aiohttp
import asyncio
from bs4 import BeautifulSoup
from pathlib import Path
import json
from datetime import datetime
import re
import time
import random
import logging
from typing import Dict, List, Optional, Tuple
import hashlib

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SECCompliantScraper:
    def __init__(self):
        self.base_url = "https://www.sec.gov"
        
        # SEC REQUIRES proper identification with email
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 HedgeIntelligence/1.0 (admin@hedgeintelligence.ai)',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Host': 'www.sec.gov'
        }
        
        self.data_dir = Path("data/sec_documents")
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # SEC rate limit: Be conservative as requested
        self.rate_limit_delay = 10.0  # 1 request every 10 seconds
        self.last_request_time = 0
        
        # Document types to download (in priority order)
        self.priority_forms = ['S-1', 'S-1/A', '10-K', '10-Q', '8-K', 'DEF 14A', '424B', 'S-3', 'S-4', 'S-8']
        self.all_forms = self.priority_forms + ['20-F', '11-K', '3', '4', '5', 'SC 13G', 'SC 13D', 'F-1']
        
    async def wait_for_rate_limit(self):
        """Enforce rate limiting with exact timing"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        if time_since_last < self.rate_limit_delay:
            wait_time = self.rate_limit_delay - time_since_last
            logger.info(f"‚è≥ Rate limiting: waiting {wait_time:.1f} seconds...")
            await asyncio.sleep(wait_time)
        
        self.last_request_time = time.time()
    
    async def fetch_with_retry(self, session: aiohttp.ClientSession, url: str, max_retries: int = 3) -> Optional[str]:
        """Fetch URL with retry logic and proper error handling"""
        for attempt in range(max_retries):
            try:
                await self.wait_for_rate_limit()
                
                logger.debug(f"Fetching: {url}")
                
                async with session.get(url, headers=self.headers, allow_redirects=True) as response:
                    if response.status == 200:
                        content = await response.text()
                        return content
                    elif response.status == 403:
                        logger.warning(f"403 Forbidden - Waiting {30 * (attempt + 1)} seconds...")
                        await asyncio.sleep(30 * (attempt + 1))
                    elif response.status == 429:
                        logger.warning(f"Rate limited - Waiting {60 * (attempt + 1)} seconds...")
                        await asyncio.sleep(60 * (attempt + 1))
                    else:
                        logger.warning(f"HTTP {response.status} for {url}")
                        await asyncio.sleep(5)
                        
            except Exception as e:
                logger.error(f"Error fetching {url}: {e}")
                await asyncio.sleep(5)
        
        return None
    
    def is_valid_document(self, content: str, filename: str) -> bool:
        """Check if the document is valid and not junk"""
        if not content:
            return False
            
        content_lower = content.lower()
        
        # Skip if too small
        if len(content) < 1000:
            logger.debug(f"Skipping {filename}: Too small ({len(content)} bytes)")
            return False
        
        # Skip error pages
        error_indicators = [
            'this page contains the following errors',
            'companysearch',
            'browse companies',
            'search results',
            'no matching companies',
            'invalid parameter',
            '404 not found',
            'access denied'
        ]
        
        for indicator in error_indicators:
            if indicator in content_lower:
                logger.debug(f"Skipping {filename}: Contains '{indicator}'")
                return False
        
        # Must contain some financial/legal content
        required_indicators = [
            'securities and exchange commission',
            'united states',
            'form',
            'pursuant to',
            'registration',
            'report',
            'financial',
            'statement'
        ]
        
        if not any(indicator in content_lower for indicator in required_indicators):
            logger.debug(f"Skipping {filename}: No financial/legal content found")
            return False
        
        return True
    
    async def get_filing_documents(self, session: aiohttp.ClientSession, filing_detail_url: str) -> List[Dict]:
        """Get actual document links from a filing detail page"""
        documents = []
        
        content = await self.fetch_with_retry(session, filing_detail_url)
        if not content:
            return documents
            
        soup = BeautifulSoup(content, 'html.parser')
        
        # Look for the document table
        doc_table = soup.find('table', class_='tableFile')
        if not doc_table:
            logger.warning(f"No document table found on {filing_detail_url}")
            return documents
        
        rows = doc_table.find_all('tr')[1:]  # Skip header
        
        for row in rows:
            cells = row.find_all('td')
            if len(cells) >= 3:
                # Get document info
                doc_name = cells[1].text.strip() if len(cells) > 1 else ""
                doc_type = cells[2].text.strip() if len(cells) > 2 else ""
                
                # Find the document link
                doc_link = row.find('a', href=True)
                if doc_link and '/Archives/edgar/data/' in doc_link['href']:
                    doc_url = f"{self.base_url}{doc_link['href']}"
                    
                    # Skip index files and exhibits we don't need
                    if 'index' in doc_name.lower() or doc_name.startswith('EX-'):
                        continue
                    
                    documents.append({
                        'name': doc_name,
                        'type': doc_type,
                        'url': doc_url
                    })
        
        return documents
    
    async def scan_and_download_everything(self, ticker: str, cik: str) -> Dict:
        """Main download function using the correct browse endpoint"""
        ticker = ticker.upper()
        cik = str(cik).lstrip('0')  # Remove leading zeros for browse URL
        
        logger.info(f"\n{'='*60}")
        logger.info(f"üîç SEC-Compliant Download for {ticker} (CIK: {cik})")
        logger.info(f"{'='*60}")
        logger.info("‚è≥ Using conservative rate limiting (1 request/10 seconds)")
        
        # Create ticker directory
        ticker_dir = self.data_dir / ticker
        ticker_dir.mkdir(exist_ok=True)
        
        # Track what we download
        downloaded_forms = {}
        total_files = 0
        errors = []
        
        # Create session
        timeout = aiohttp.ClientTimeout(total=60)
        connector = aiohttp.TCPConnector(limit=1, force_close=True)
        
        async with aiohttp.ClientSession(timeout=timeout, connector=connector) as session:
            # Use the correct browse URL
            browse_url = f"{self.base_url}/edgar/browse/?CIK={cik}"
            
            logger.info(f"\nüì° Fetching filings from: {browse_url}")
            
            # Get the browse page
            browse_content = await self.fetch_with_retry(session, browse_url)
            
            if not browse_content:
                return {
                    'success': False,
                    'error': 'Could not access SEC browse page',
                    'ticker': ticker,
                    'cik': cik
                }
            
            # Parse the browse page
            soup = BeautifulSoup(browse_content, 'html.parser')
            
            # Extract company info
            company_info_elem = soup.find('span', class_='companyName')
            company_name = company_info_elem.text.strip() if company_info_elem else ticker
            
            # Find the filings table (after "View filings" section)
            # Look for the table with filing information
            filings_table = None
            tables = soup.find_all('table')
            
            for table in tables:
                # Check if this table has filing data
                first_row = table.find('tr')
                if first_row and any(text in str(first_row) for text in ['Filing Date', 'Form', 'Description']):
                    filings_table = table
                    break
            
            if not filings_table:
                logger.error("Could not find filings table on browse page")
                return {
                    'success': False,
                    'error': 'No filings table found',
                    'ticker': ticker,
                    'cik': cik
                }
            
            # Save company info
            company_info = {
                'ticker': ticker,
                'cik': cik,
                'name': company_name,
                'last_updated': datetime.now().isoformat()
            }
            
            with open(ticker_dir / 'company_info.json', 'w') as f:
                json.dump(company_info, f, indent=2)
            
            # Process each filing
            rows = filings_table.find_all('tr')[1:]  # Skip header
            logger.info(f"Found {len(rows)} filings")
            
            for i, row in enumerate(rows):
                cells = row.find_all('td')
                if len(cells) >= 3:
                    # Extract filing info
                    filing_date = cells[0].text.strip()
                    form_type = cells[1].text.strip()
                    
                    # Skip if not a form we want
                    if form_type not in self.all_forms:
                        continue
                    
                    # Find the filing detail link
                    filing_link = None
                    for link in row.find_all('a', href=True):
                        if '/Archives/edgar/data/' in link['href'] and link['href'].endswith('-index.htm'):
                            filing_link = f"{self.base_url}{link['href']}"
                            break
                    
                    if not filing_link:
                        continue
                    
                    logger.info(f"\n[{i+1}/{len(rows)}] Processing {form_type} filed on {filing_date}")
                    
                    # Get documents from this filing
                    documents = await self.get_filing_documents(session, filing_link)
                    
                    for doc in documents:
                        # Download the document
                        doc_content = await self.fetch_with_retry(session, doc['url'])
                        
                        if doc_content and self.is_valid_document(doc_content, doc['name']):
                            # Generate filename
                            date_str = filing_date.replace('-', '').replace('/', '')
                            doc_hash = hashlib.md5(doc['url'].encode()).hexdigest()[:6]
                            filename = f"{form_type}_{date_str}_{doc['name']}_{doc_hash}.html"
                            filename = re.sub(r'[^a-zA-Z0-9._-]', '_', filename)
                            
                            filepath = ticker_dir / filename
                            
                            # Save document
                            with open(filepath, 'w', encoding='utf-8') as f:
                                f.write(doc_content)
                            
                            # Save metadata
                            metadata = {
                                'form_type': form_type,
                                'filing_date': filing_date,
                                'document_name': doc['name'],
                                'document_type': doc['type'],
                                'download_date': datetime.now().isoformat(),
                                'source_url': doc['url'],
                                'company_name': company_name
                            }
                            
                            metadata_path = filepath.with_suffix('.json')
                            with open(metadata_path, 'w') as f:
                                json.dump(metadata, f, indent=2)
                            
                            logger.info(f"   ‚úÖ Downloaded: {filename} ({len(doc_content)//1024} KB)")
                            
                            downloaded_forms[form_type] = downloaded_forms.get(form_type, 0) + 1
                            total_files += 1
                        else:
                            logger.info(f"   ‚ö†Ô∏è  Skipped invalid document: {doc['name']}")
            
            # Save summary
            summary = {
                'ticker': ticker,
                'cik': cik,
                'company_name': company_name,
                'last_scan': datetime.now().isoformat(),
                'total_files': total_files,
                'forms_downloaded': downloaded_forms,
                'scan_version': '8.0-browse-fixed'
            }
            
            with open(ticker_dir / "scan_summary.json", 'w') as f:
                json.dump(summary, f, indent=2)
            
            logger.info(f"\n{'='*60}")
            logger.info(f"‚úÖ Download Complete!")
            logger.info(f"üìä Total valid files: {total_files}")
            logger.info(f"üìÅ Forms downloaded: {downloaded_forms}")
            logger.info(f"{'='*60}")
            
            return {
                'success': True,
                'ticker': ticker,
                'cik': cik,
                'total_files': total_files,
                'forms_downloaded': downloaded_forms,
                'summary': summary
            }

# Also expose as EnhancedSECDocumentScraper for compatibility
EnhancedSECDocumentScraper = SECCompliantScraper

# Quick test
async def test_crcl():
    """Test with Circle (CRCL)"""
    scraper = SECCompliantScraper()
    result = await scraper.scan_and_download_everything("CRCL", "0001876042")
    logger.info(f"\nResult: {json.dumps(result, indent=2)}")

if __name__ == "__main__":
    asyncio.run(test_crcl())